{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b84774c",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks(CNN) 개념\n",
    "- (conv - relu - conv - relu - pool) 하나의 구조로 이루어져 있고 이 구조를 여러번 반복한 다음 FC(fulling Networks)로 분류하는 구조\n",
    "- conv란 전체 픽셀에서 일부 픽셀 값만 불러서 가져오는 것을 convolution이라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3961b",
   "metadata": {},
   "source": [
    "# 32 * 32 * 3의 image가 있다고 가정\n",
    "- 이미지의 일부의 픽셀 값을 처리하는 과정을 filter하며 역할은 일부 픽셀의 값을 정의함\n",
    "- conv란 수 많은 X1W1 + X2W2 + X3W3 + .... + XnWn이 모인 값을 의미함\n",
    "- 수 많은 수식을 합하여 공식화 하면 hypothesis = tf.matmul(X, W) + b와 같은 conv가 만들어진다고 볼 수 있음(2일차 multi-variable 부분 참조)\n",
    "- relu((W * hypothesis) + b)를 하는 것으로 conv한 값을 단순히 relu에 넣어주는 것을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c29cb",
   "metadata": {},
   "source": [
    "# 실제로 CNN 활용 시 W 등 설정 하는 방식(7 * 7 사이즈 이미지 예시)\n",
    "- kernel / stride 값에 따라 수식이 변화될 수 있음\n",
    "- kernel = 3, stride = 2 -> (input - kernel) / stride + 1 = (7-3)/2 + 1 = 3\n",
    "- size가 줄어드는 것을 방지하기 위해 사용하는 것이 padding인데 (kernel - 1)/2를 하면 기존의 image 값을 보존 할 수 있음\n",
    "- 위의 예를 패딩에 적용할 경우 (3-1)/2 = 1의 padding을 주면 됨\n",
    "- padding 했을 경우의 값까지 구하고 싶다면 (input - kernel + 2 * padding) / stride + 1를 하면 padding까지 고려한 output 설정 가능\n",
    "- 따라서 layer층을 만들고 W를 설정할 시에는 kernel 값에 filter을 해주면 됨\n",
    "- 위의 경우 첫번째 W의 경우 (3 * 3)(부분 추출 범위) * 3(RGB) * 6(filter) = 3 * 3 * 3 * 6 = 162의 값을 넘겨줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82549d9",
   "metadata": {},
   "source": [
    "# 처음 CNN 나왔을 당시의 공식( 5 * 5 stride = 1 -> 2 * 2 stride =2 반복)\n",
    "- 32 * 32 이미지를 5 * 5에 stride = 1를 주고 filter = 6를 주어 적용\n",
    "- 28 * 28(5*5 stride = 1 이므로 (32-5)/1 + 1 = 28)\n",
    "- 28 * 28 이미지를 2 * 2에 stride =2를 주고 filter = 6를 주어 적용\n",
    "- 14 * 14(2*2 stride = 2 이므로 (28-2)2 + 1 = 14)\n",
    "- 14 * 14 이미지를 5 * 5에 stride =1를 주고 filter = 16를 주어 적용\n",
    "- 10 * 10(5*5 stride = 1 이므로 (14-5)1 + 1 = 10)\n",
    "- 10 * 10 이미지를 2 * 2에 stride =2를 주고 filter = 6를 주어 적용\n",
    "- 5 * 5(2*2 stride = 2 이므로 (10-2)2 + 1 = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1a7ba",
   "metadata": {},
   "source": [
    "# CNN 수정(Alex Net - 전체 구조)\n",
    "- 세부사항(relu 처음 사용 / dropout = 0.5 / batch_size =128 / 동일한 networks 7개를 ensemble 하여 실행)\n",
    "\n",
    "\n",
    "- 227 * 227 이미지를 11 * 11에 stride = 4를 주고 filter = 96를 주어 적용\n",
    "- 55 * 55(11*11 stride = 4  이므로 (227-11)/4 + 1 = 55)\n",
    "- 55 * 55 이미지를 3 * 3에 stride = 2를 주고 filter = 96를 주어 maxpool적용\n",
    "- 27 * 27(3*3 stride = 2 이므로 (55-3)/2 +1 = 27)\n",
    "- 27 * 27(Normalization하여 오차 역전파을 평준화 해줌)\n",
    "\n",
    "- 27 * 27 이미지를 5 * 5에 stride = 1, padding = 2, filter = 256를 주어 적용\n",
    "- 27 * 27(5*5 stride=1, padding=2 이므로 (27-5+2*2)1+1= 27)\n",
    "- 27 * 27 이미지를 3 * 3에 stride = 2를 주고 filter = 256를 주어 maxpool적용\n",
    "- 13 * 13(3*3 stride=2, padding=2 이므로 (27-3)2+1= 13)\n",
    "- 13 * 13(Normalization하여 오차 역전파을 평준화 해줌)\n",
    "\n",
    "- 13 * 13 이미지를 3 * 3에 stride = 1, padding = 1, filter = 384를 주어 적용\n",
    "- 13 * 13(3*3 stride=1, padding=1 이므로 (13-3+2*1)1+1= 13)\n",
    "- 13 * 13 이미지를 3 * 3에 stride = 1, padding = 1, filter = 384를 주어 적용\n",
    "- 13 * 13(3*3 stride=1, padding=1 이므로 (13-3+2*1)1+1= 13)\n",
    "- 13 * 13 이미지를 3 * 3에 stride = 1, padding = 1, filter = 256를 주어 적용\n",
    "- 13 * 13(3*3 stride=1, padding=1 이므로 (13-3+2*1)1+1= 13)\n",
    "- 13 * 13 이미지를 3 * 3에 stride =2를 주고 filter = 256을 주어 maxpool 적용\n",
    "- 6 * 6((3*3 stride = 2 이므로 (13-3)/2 +1 = 6)\n",
    "- FC6: 4096 neurons\n",
    "- FC7: 4096 neurons\n",
    "- FC8: 1000 neurons(class scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40bf7c",
   "metadata": {},
   "source": [
    "# 모델 학습 과정에서 layer층이 늘어날 경우 대응 방안\n",
    "- ResNet의 경우 기존 AlexNet(layer=8), googleNet(layer=19)에 비해 비약적인 152개의 layer층으로 구성되어 있음\n",
    "- 이럴 경우 학습하는 과정이 너무 길어지기 때문에 fastforword라는 개념을 활용하여 layer층들을 더하여 구성\n",
    "- fastforword란 일부 layer층에 가중치를 전달하는 것을 건너띄고 다음 layer층과 함께 가중치를 합하여 표현하는 방식\n",
    "- layer층이 많지만 실질적으로는 layer층이 건너띄는 layer층 만큼 줄어드는 효과가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92621e9c",
   "metadata": {},
   "source": [
    "# CNN 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a712db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\since\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9f7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbadc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array([[[[1],[2],[3],[10]],[[4],[5],[6],[11]],[[7],[8],[9],[12]]]], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1d519e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 4, 1)\n",
      "image[0]: batch_size\n",
      "image[1]: image_width\n",
      "image[2]: image_height\n",
      "image[3]: image_color\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(image.shape)\n",
    "print(\"image[0]: {}\\nimage[1]: {}\\nimage[2]: {}\\nimage[3]: {}\\n\".format\n",
    "      (\"batch_size\", \"image_width\", \"image_height\", \"image_color\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1aa3c",
   "metadata": {},
   "source": [
    "###### 아래를 위의 값을 넣는다고 가정하면\n",
    "- 1/2/3/10\n",
    "- 4/5/6/11\n",
    "- 7/8/9/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a40aa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17b9beb1e80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.reshape(3,4), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf9572",
   "metadata": {},
   "source": [
    "###### weight = (2,2,1,1)로 주는데 앞의 2,2는 kernel 값이고 다음 1은 RGB 그리고 마지막 1은 stride를 의미\n",
    "- padding = same(image와 동일한 size) / valid(padding 없이 유효한 영역만 사용)\n",
    "- padding을 SAME으로 줄 경우 W에 관계 없이 자동적으로 이미지 규격에 맞게 사이즈 맞춰줌\n",
    "- padding을 VALID로 줄 경우 위의 W 값을 잘 적용해서 주어야 제대로 W에 맞게 사이즈가 출력됨\n",
    "- 아래의 W는 2,2씩 움직이므로 최종 2,3의 값을 가진 값이 나오게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ab364ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 4, 1)\n",
      "weight.shape (1, 1, 1, 1)\n",
      "conv2d_img.shape (1, 3, 4, 1)\n",
      "[[ 1.  2.  3. 10.]\n",
      " [ 4.  5.  6. 11.]\n",
      " [ 7.  8.  9. 12.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH4AAABpCAYAAAD88JerAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEH0lEQVR4nO3dwWscdRjG8e9jmp4SeqiSlDZoDkXoTSgl4E0wRC/12Bw8CZ4KCl76V3jzErAIRSyCHnoolB4EEUQSiwdjqYSCNERQMaA9tYXXQ0NYdZtMkn1ndnifDwQyGfLbd/bhN7v723d2FRFYPc91XYB1w8EX5eCLcvBFOfiiHHxRxzIGlZT+GvHYsZTSd01OTqaODzA/P586/tbWFtvb2xq2L/feS3Ty5MnU8WdmZlLHB7h27Vrq+MvLy8/c51N9UQ6+KAdflIMvysEX5eCLcvBFNQpe0pKke5I2JF3JLsry7Ru8pAngI+AN4BywLOlcdmGWq8mMvwBsRMT9iHgEXAcu5pZl2ZoEfxp4MLC9ufO3f5H0rqQ1SWujKs7yNFmrH7bI/783YSJiBViBdt6ksaNpMuM3gbmB7TPAVk451pYmwa8CZyXNSzoOXAJu5JZl2fY91UfEE0mXgVvABHA1ItbTK7NUjd6Pj4ibwM3kWqxFXrkrysEX5eCLcvBFOfiiHHxRDr6olL766elpFhYWMobeNTs7mzp+G3312bex10UnnvFFOfiiHHxRDr4oB1+Ugy/KwRfVpL36qqTfJP3YRkHWjiYz/hNgKbkOa9m+wUfE18CfLdRiLfJjfFEjC37wgorHjx+PalhLMrLgI2IlIs5HxPk2PjHKjsan+qKavJz7DPgWeFnSpqR38suybE0uqHj2h6VZb/lUX5SDL8rBF+Xgi3LwRTn4ohx8USl99SdOnGBxcTFj6F3ZPenZffuQfwx7LZ17xhfl4Ity8EU5+KIcfFEOvigHX5SDL6pJB86cpK8k3ZW0Lum9NgqzXE1W7p4AH0TEHUnTwPeSbkfET8m1WaImF1T8GhF3dn7/G7jLkM+rt3450GO8pJeAV4Dvhuzb7at/+PDhiMqzLI2DlzQFfAG8HxF//Xf/YF/91NTUKGu0BE2/hWqSp6F/GhFf5pZkbWjyrF7Ax8DdiPgwvyRrQ5MZ/yrwNvCapB92ft5MrsuSNbmg4huGfyGR9ZhX7opy8EU5+KIcfFEOvigHX5SDL0oRo//+X0m/A78c4F+eB/4YeSHtGsdjeDEiXhi2IyX4g5K0FhHnu67jKPp2DD7VF+XgixqX4Fe6LmAEenUMY/EYb+0blxlvLes0eElLku5J2pB0pctaDquv7eedneolTQA/A68Dm8AqsNy3tm1Jp4BTg+3nwFvjfhxdzvgLwEZE3I+IR8B14GKH9RxKX9vPuwz+NPBgYHuTHtxhe9mr/XzcdBn8sHau3r7E2K/9fNx0GfwmMDewfQbY6qiWI+lj+3mXwa8CZyXNSzoOXAJudFjPofS1/byz4CPiCXAZuMXTJ0SfR8R6V/UcQS/bz71yV5RX7opy8EU5+KIcfFEOvigHX5SDL8rBF/UPcNYJOsNta3QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"image.shape\", image.shape)\n",
    "# padding = SAME 시\n",
    "#weight_filter3 = tf.constant([[[[1., 10, -1]], [[1., 10, -1]]],[[[1., 10, -1]],[[1., 10, -1]]]])\n",
    "# padding = VAILD 시\n",
    "# kernel = 1 일 때\n",
    "weight_filter3 = tf.constant([[[[1.]]]])\n",
    "# kernel = 2 일 때\n",
    "#weight_filter3 = tf.constant([[[[1.]],[[1.]]],[[[1.]],[[1.]]]])\n",
    "print(\"weight.shape\", weight_filter3.shape)\n",
    "# padding 값 다르게 주기 (same/valid)\n",
    "# w 값 filter 다르게 주기\n",
    "conv2d = tf.nn.conv2d(image, weight_filter3, strides=[1,1,1,1], padding=\"VALID\")\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    #padding에 따라 reshpae가 달라짐 - same(3,3) / valid(2,2)\n",
    "    print(one_img.reshape(3,4))\n",
    "    plt.subplot(1,3,i+1), plt.imshow(one_img.reshape(3,4), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "320916bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array([[[[4],[3]],[[2],[1]]]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4fdb60",
   "metadata": {},
   "source": [
    "###### padding은 동일하고 각각의 값 4, 3, 2, 1  값 중에서 가장 큰 값을 pool해오므로 아래와 같이 4, 3, 2, 1의 값을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f7a48ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n",
      "[[[[4.]\n",
      "   [3.]]\n",
      "\n",
      "  [[2.]\n",
      "   [1.]]]]\n"
     ]
    }
   ],
   "source": [
    "pool = tf.nn.max_pool(image, ksize=[1,2,2,1],strides=[1,1,1,1], padding=\"SAME\")\n",
    "print(pool.shape)\n",
    "print(pool.eval())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
